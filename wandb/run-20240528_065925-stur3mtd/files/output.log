Traceback (most recent call last):
  File "/zhome/ea/9/137501/Desktop/ML_final/ML_final/main.py", line 135, in <module>
    Defaults.start()
  File "/zhome/ea/9/137501/Desktop/ML_final/project-env/lib/python3.11/site-packages/dtu/__init__.py", line 235, in start
    cls.run(*args)
  File "/zhome/ea/9/137501/Desktop/ML_final/ML_final/main.py", line 82, in run
    text_logits, _ = model.apply({"params": params}, imgs, txts[:, :-1], mask_ar[:, :-1], train=True)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/zhome/ea/9/137501/Desktop/ML_final/ML_final/big_vision/models/proj/paligemma/paligemma.py", line 151, in __call__
    _, out_llm = self._llm(x, mask=attn_mask, train=train)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/zhome/ea/9/137501/Desktop/ML_final/ML_final/big_vision/models/proj/paligemma/gemma_bv.py", line 81, in __call__
    logits, out = self.model(
                  ^^^^^^^^^^^
  File "/zhome/ea/9/137501/Desktop/ML_final/ML_final/big_vision/models/ppp/gemma.py", line 461, in __call__
    x = embedder.decode(x)
        ^^^^^^^^^^^^^^^^^^
  File "/zhome/ea/9/137501/Desktop/ML_final/ML_final/big_vision/models/ppp/gemma.py", line 175, in decode
    return jnp.dot(x, self.input_embedding_table.T)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 2110783616 bytes.
BufferAssignment OOM Debugging.
BufferAssignment stats:
             parameter allocation:    1.02GiB
              constant allocation:         0B
        maybe_live_out allocation:    4.89GiB
     preallocated temp allocation:    1.97GiB
  preallocated temp fragmentation:         0B (0.00%)
                 total allocation:    7.88GiB
              total fragmentation:       112B (0.00%)
Peak buffers:
	Buffer 1:
		Size: 4.89GiB
		Operator: op_name="jit(dot)/jit(main)/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=float32]" source_file="/zhome/ea/9/137501/Desktop/ML_final/ML_final/big_vision/models/ppp/gemma.py" source_line=175
		XLA Label: custom-call
		Shape: f32[5104,257152]
		==========================
	Buffer 2:
		Size: 1.96GiB
		Operator: op_name="jit(dot)/jit(main)/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=float32]" source_file="/zhome/ea/9/137501/Desktop/ML_final/ML_final/big_vision/models/ppp/gemma.py" source_line=175
		XLA Label: fusion
		Shape: f32[2048,257152]
		==========================
	Buffer 3:
		Size: 1004.50MiB
		Entry Parameter Subshape: f16[2048,257152]
		==========================
	Buffer 4:
		Size: 39.88MiB
		Entry Parameter Subshape: f32[16,319,2048]
		==========================
	Buffer 5:
		Size: 4.00MiB
		Operator: op_name="jit(dot)/jit(main)/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=float32]" source_file="/zhome/ea/9/137501/Desktop/ML_final/ML_final/big_vision/models/ppp/gemma.py" source_line=175
		XLA Label: custom-call
		Shape: s8[4194304]
		==========================
	Buffer 6:
		Size: 16B
		Operator: op_name="jit(dot)/jit(main)/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=float32]" source_file="/zhome/ea/9/137501/Desktop/ML_final/ML_final/big_vision/models/ppp/gemma.py" source_line=175
		XLA Label: custom-call
		Shape: (f32[5104,257152], s8[4194304])
		==========================
--------------------
For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.