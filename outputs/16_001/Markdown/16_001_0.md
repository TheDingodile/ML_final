2024-05-23 23:10:42.441806: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
wandb: Currently logged in as: kobomao. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /zhome/ea/9/137501/Desktop/ML_final/ML_final/wandb/run-20240523_231053-xucvxbyl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 16_001-0
wandb: ‚≠êÔ∏è View project at https://wandb.ai/kobomao/ML_healthcare
wandb: üöÄ View run at https://wandb.ai/kobomao/ML_healthcare/runs/xucvxbyl
2024-05-23 23:11:23.779120: W external/xla/xla/service/gpu/nvptx_compiler.cc:760] The NVIDIA driver's CUDA version is 12.4 which is older than the ptxas CUDA version (12.5.40). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.
2024-05-23 23:11:31.019002: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2024-05-23 23:12:12.042951: W external/xla/xla/service/hlo_rematerialization.cc:2948] Can't reduce memory use below 12.25GiB (13158860141 bytes) by rematerialization; only reduced to 22.86GiB (24541302272 bytes), down from 22.90GiB (24594307344 bytes) originally
2024-05-23 23:12:25.905232: W external/tsl/tsl/framework/bfc_allocator.cc:482] Allocator (GPU_0_bfc) ran out of memory trying to allocate 11.99GiB (rounded to 12874361856)requested by op 
2024-05-23 23:12:25.905980: W external/tsl/tsl/framework/bfc_allocator.cc:494] ********************_________*****************************__________________________________________
E0523 23:12:25.907730   21880 pjrt_stream_executor_client.cc:2826] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 12874361776 bytes.
BufferAssignment OOM Debugging.
BufferAssignment stats:
             parameter allocation:   10.90GiB
              constant allocation:        12B
        maybe_live_out allocation:   10.89GiB
     preallocated temp allocation:   11.99GiB
                 total allocation:   22.89GiB
Peak buffers:
	Buffer 1:
		Size: 4.50GiB
		XLA Label: fusion
		Shape: f32[18,2,2048,16384]
		==========================

	Buffer 2:
		Size: 4.50GiB
		Operator: op_name="jit(update_fn)/jit(main)/sub" source_file="/zhome/ea/9/137501/Desktop/ML_final/ML_final/main.py" source_line=62
		XLA Label: fusion
		Shape: f32[18,2,2048,16384]
		==========================

	Buffer 3:
		Size: 2.25GiB
		XLA Label: fusion
		Shape: f32[18,16384,2048]
		==========================

	Buffer 4:
		Size: 2.25GiB
		Operator: op_name="jit(update_fn)/jit(main)/sub" source_file="/zhome/ea/9/137501/Desktop/ML_final/ML_final/main.py" source_line=62
		XLA Label: fusion
		Shape: f32[18,16384,2048]
		==========================

	Buffer 5:
		Size: 1.96GiB
		Operator: op_name="jit(update_fn)/jit(main)/sub" source_file="/zhome/ea/9/137501/Desktop/ML_final/ML_final/main.py" source_line=62
		XLA Label: fusion
		Shape: f32[257152,2048]
		==========================

	Buffer 6:
		Size: 988.80MiB
		Operator: op_name="jit(update_fn)/jit(main)/jvp(Model)/llm.compute_logits/Model/embedder.decode/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=float32]" source_file="/zhome/ea/9/137501/Desktop/ML_final/ML_final/big_vision/models/ppp/gemma.py" source_line=175
		XLA Label: custom-call
		Shape: f32[1008,257152]
		==========================

	Buffer 7:
		Size: 510.68MiB
		Operator: op_name="jit(update_fn)/jit(main)/sub" source_file="/zhome/ea/9/137501/Desktop/ML_final/ML_final/main.py" source_line=62
		XLA Label: fusion
		Shape: f32[27,4304,1152]
		==========================

	Buffer 8:
		Size: 510.68MiB
		Operator: op_name="jit(update_fn)/jit(main)/sub" source_file="/zhome/ea/9/137501/Desktop/ML_final/ML_final/main.py" source_line=62
		XLA Label: fusion
		Shape: f32[27,1152,4304]
		==========================

	Buffer 9:
		Size: 288.00MiB
		XLA Label: fusion
		Shape: f32[18,8,2048,256]
		==========================

	Buffer 10:
		Size: 288.00MiB
		XLA Label: fusion
		Shape: f32[18,8,256,2048]
		==========================

	Buffer 11:
		Size: 288.00MiB
		Operator: op_name="jit(update_fn)/jit(main)/sub" source_file="/zhome/ea/9/137501/Desktop/ML_final/ML_final/main.py" source_line=62
		XLA Label: fusion
		Shape: f32[18,8,2048,256]
		==========================

	Buffer 12:
		Size: 288.00MiB
		Operator: op_name="jit(update_fn)/jit(main)/sub" source_file="/zhome/ea/9/137501/Desktop/ML_final/ML_final/main.py" source_line=62
		XLA Label: fusion
		Shape: f32[18,8,256,2048]
		==========================

	Buffer 13:
		Size: 136.69MiB
		Operator: op_name="jit(update_fn)/jit(main)/sub" source_file="/zhome/ea/9/137501/Desktop/ML_final/ML_final/main.py" source_line=62 deduplicated_name="loop_subtract_fusion.8"
		XLA Label: fusion
		Shape: f32[27,1152,16,72]
		==========================

	Buffer 14:
		Size: 136.69MiB
		Operator: op_name="jit(update_fn)/jit(main)/sub" source_file="/zhome/ea/9/137501/Desktop/ML_final/ML_final/main.py" source_line=62 deduplicated_name="loop_subtract_fusion.8"
		XLA Label: fusion
		Shape: f32[27,1152,16,72]
		==========================

	Buffer 15:
		Size: 136.69MiB
		Operator: op_name="jit(update_fn)/jit(main)/sub" source_file="/zhome/ea/9/137501/Desktop/ML_final/ML_final/main.py" source_line=62
		XLA Label: fusion
		Shape: f32[27,16,72,1152]
		==========================



<style>
c { color: #9cdcfe; font-family: 'Verdana', sans-serif;} /* VARIABLE */
d { color: #4EC9B0; font-family: 'Verdana', sans-serif;} /* CLASS */
e { color: #569cd6; font-family: 'Verdana', sans-serif;} /* BOOL */
f { color: #b5cea8; font-family: 'Verdana', sans-serif;} /* NUMBERS */
j { color: #ce9178; font-family: 'Verdana', sans-serif;} /* STRING */
k { font-family: 'Verdana', sans-serif;} /* SYMBOLS */
</style>

# Parameters

| PARAMETER         | TYPE              | VALUE             |
|-------------------|-------------------|-------------------|
| <c>name</c>       | <d>str</d>        | <j>"16_001-0"</j> |
| <c>time</c>       | <d>int</d>        | <f>3600</f>       |
| <c>lr</c>         | <d>float</d>      | <f>0.01</f>       |
| <c>batch_size</c> | <d>int</d>        | <f>16</f>         |
| <c>steps</c>      | <d>int</d>        | <f>5000</f>       |

# Output

```
Traced<ShapedArray(float32[16,63])>with<JVPTrace(level=3/0)> with
  primal = Traced<ShapedArray(float32[16,63])>with<DynamicJaxprTrace(level=1/0)>
  tangent = Traced<ShapedArray(float32[16,63])>with<JaxprTrace(level=2/0)> with
    pval = (ShapedArray(float32[16,63]), None)
    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7fa8cc1caa70>, in_tracers=(Traced<ShapedArray(float32[16,63,257152]):JaxprTrace(level=2/0)>, Traced<ShapedArray(float32[16,63,257152]):JaxprTrace(level=2/0)>, Traced<ShapedArray(float32[16,63]):JaxprTrace(level=2/0)>), out_tracer_refs=[<weakref at 0x7fa8cc049850; to 'JaxprTracer' at 0x7fa8cc048860>], out_avals=[ShapedArray(float32[16,63])], primitive=pjit, params={'jaxpr': { lambda ; a:f32[16,63,257152] b:f32[16,63,257152] c:f32[16,63]. let
    d:f32[16,63,257152] = mul a b
    e:f32[16,63] = reduce_sum[axes=(2,)] d
    f:f32[16,63] = div e c
  in (f,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'in_layouts': (None, None, None), 'out_layouts': (None,), 'resource_env': None, 'donated_invars': (False, False, False), 'name': '_reduce_max', 'keep_unused': False, 'inline': True}, effects=set(), source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x136995d0>, name_stack=NameStack(stack=(Transform(name='jvp'),)))) (16, 63)
Traceback (most recent call last):
  File "/zhome/ea/9/137501/Desktop/ML_final/ML_final/main.py", line 98, in <module>
    Defaults.start()
  File "/zhome/ea/9/137501/Desktop/ML_final/project-env/lib/python3.11/site-packages/dtu/__init__.py", line 235, in start
    cls.run(*args)
  File "/zhome/ea/9/137501/Desktop/ML_final/ML_final/main.py", line 73, in run
    params, loss = update_fn(params, batch, learning_rate)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 12874361776 bytes.
BufferAssignment OOM Debugging.
BufferAssignment stats:
             parameter allocation:   10.90GiB
              constant allocation:        12B
        maybe_live_out allocation:   10.89GiB
     preallocated temp allocation:   11.99GiB
                 total allocation:   22.89GiB
Peak buffers:
	Buffer 1:
		Size: 4.50GiB
		XLA Label: fusion
		Shape: f32[18,2,2048,16384]
		==========================

	Buffer 2:
		Size: 4.50GiB
		Operator: op_name="jit(update_fn)/jit(main)/sub" source_file="/zhome/ea/9/137501/Desktop/ML_final/ML_final/main.py" source_line=62
		XLA Label: fusion
		Shape: f32[18,2,2048,16384]
		==========================

	Buffer 3:
		Size: 2.25GiB
		XLA Label: fusion
		Shape: f32[18,16384,2048]
		==========================

	Buffer 4:
		Size: 2.25GiB
		Operator: op_name="jit(update_fn)/jit(main)/sub" source_file="/zhome/ea/9/137501/Desktop/ML_final/ML_final/main.py" source_line=62
		XLA Label: fusion
		Shape: f32[18,16384,2048]
		==========================

	Buffer 5:
		Size: 1.96GiB
		Operator: op_name="jit(update_fn)/jit(main)/sub" source_file="/zhome/ea/9/137501/Desktop/ML_final/ML_final/main.py" source_line=62
		XLA Label: fusion
		Shape: f32[257152,2048]
		==========================

	Buffer 6:
		Size: 988.80MiB
		Operator: op_name="jit(update_fn)/jit(main)/jvp(Model)/llm.compute_logits/Model/embedder.decode/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=float32]" source_file="/zhome/ea/9/137501/Desktop/ML_final/ML_final/big_vision/models/ppp/gemma.py" source_line=175
		XLA Label: custom-call
		Shape: f32[1008,257152]
		==========================

	Buffer 7:
		Size: 510.68MiB
		Operator: op_name="jit(update_fn)/jit(main)/sub" source_file="/zhome/ea/9/137501/Desktop/ML_final/ML_final/main.py" source_line=62
		XLA Label: fusion
		Shape: f32[27,4304,1152]
		==========================

	Buffer 8:
		Size: 510.68MiB
		Operator: op_name="jit(update_fn)/jit(main)/sub" source_file="/zhome/ea/9/137501/Desktop/ML_final/ML_final/main.py" source_line=62
		XLA Label: fusion
		Shape: f32[27,1152,4304]
		==========================

	Buffer 9:
		Size: 288.00MiB
		XLA Label: fusion
		Shape: f32[18,8,2048,256]
		==========================

	Buffer 10:
		Size: 288.00MiB
		XLA Label: fusion
		Shape: f32[18,8,256,2048]
		==========================

	Buffer 11:
		Size: 288.00MiB
		Operator: op_name="jit(update_fn)/jit(main)/sub" source_file="/zhome/ea/9/137501/Desktop/ML_final/ML_final/main.py" source_line=62
		XLA Label: fusion
		Shape: f32[18,8,2048,256]
		==========================

	Buffer 12:
		Size: 288.00MiB
		Operator: op_name="jit(update_fn)/jit(main)/sub" source_file="/zhome/ea/9/137501/Desktop/ML_final/ML_final/main.py" source_line=62
		XLA Label: fusion
		Shape: f32[18,8,256,2048]
		==========================

	Buffer 13:
		Size: 136.69MiB
		Operator: op_name="jit(update_fn)/jit(main)/sub" source_file="/zhome/ea/9/137501/Desktop/ML_final/ML_final/main.py" source_line=62 deduplicated_name="loop_subtract_fusion.8"
		XLA Label: fusion
		Shape: f32[27,1152,16,72]
		==========================

	Buffer 14:
		Size: 136.69MiB
		Operator: op_name="jit(update_fn)/jit(main)/sub" source_file="/zhome/ea/9/137501/Desktop/ML_final/ML_final/main.py" source_line=62 deduplicated_name="loop_subtract_fusion.8"
		XLA Label: fusion
		Shape: f32[27,1152,16,72]
		==========================

	Buffer 15:
		Size: 136.69MiB
		Operator: op_name="jit(update_fn)/jit(main)/sub" source_file="/zhome/ea/9/137501/Desktop/ML_final/ML_final/main.py" source_line=62
		XLA Label: fusion
		Shape: f32[27,16,72,1152]
		==========================


--------------------
For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.005 MB of 0.005 MB uploadedwandb: / 0.009 MB of 0.020 MB uploaded (0.004 MB deduped)wandb: - 0.015 MB of 0.020 MB uploaded (0.004 MB deduped)wandb: \ 0.020 MB of 0.020 MB uploaded (0.004 MB deduped)wandb: üöÄ View run 16_001-0 at: https://wandb.ai/kobomao/ML_healthcare/runs/xucvxbyl
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/kobomao/ML_healthcare
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240523_231053-xucvxbyl/logs

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 21839145: <16_001_0> in cluster <dcc> Exited

Job <16_001_0> was submitted from host <n-62-30-2> by user <s183914> in cluster <dcc> at Thu May 23 09:29:10 2024
Job was executed on host(s) <4*n-62-11-13>, in queue <gpuv100>, as user <s183914> in cluster <dcc> at Thu May 23 23:09:59 2024
</zhome/ea/9/137501> was used as the home directory.
</zhome/ea/9/137501/Desktop/ML_final/ML_final> was used as the working directory.
Started at Thu May 23 23:09:59 2024
Terminated at Thu May 23 23:12:45 2024
Results reported at Thu May 23 23:12:45 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/sh
#BSUB -q gpuv100
#BSUB -gpu "num=1:mode=exclusive_process"
#BSUB -n 4
#BSUB -R "rusage[mem=16G]"
#BSUB -R "select[gpu32gb]"
#BSUB -R "span[hosts=1]"
#BSUB -W 1440
# end of BSUB options
module -s load python3
source ../project-env/bin/activate

python main.py $MYARGS
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   76.97 sec.
    Max Memory :                                 2472 MB
    Average Memory :                             1878.75 MB
    Total Requested Memory :                     65536.00 MB
    Delta Memory :                               63064.00 MB
    Max Swap :                                   -
    Max Processes :                              6
    Max Threads :                                71
    Run time :                                   166 sec.
    Turnaround time :                            49415 sec.

The output (if any) is above this job summary.

